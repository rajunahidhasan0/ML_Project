{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218aeb79",
   "metadata": {
    "id": "218aeb79"
   },
   "source": [
    "## Predication with Different Classification Method to The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8u87wItZgc5z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3655,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "8u87wItZgc5z",
    "outputId": "4dd02dd2-60c8-4f40-ffcc-e77d434ece33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikeras) (3.7.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikeras) (1.5.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.3)\n",
      "Requirement already satisfied: rich in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras>=3.2.0->scikeras) (24.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nahid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cecb0a47",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "cecb0a47"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df5f1c92",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "df5f1c92"
   },
   "outputs": [],
   "source": [
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b276802",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "4b276802",
    "outputId": "dffe4664-6b45-41f0-87d7-890651faed74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(obj, p, cycle)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set max output lines before scrolling\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.instance().display_formatter.formatters['text/plain'].for_type(\n",
    "    type, lambda obj, p, cycle: p.text(repr(obj)[:10000])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cad",
   "metadata": {
    "id": "db941cad"
   },
   "source": [
    "### Metrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1aceecb7",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "1aceecb7"
   },
   "outputs": [],
   "source": [
    "#Metric Calculations\n",
    "\n",
    "def calculate_metrics(classifier, y_val, y_pred):\n",
    "    print(f\"{classifier} metrics: \")\n",
    "\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9677b8a",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "c9677b8a"
   },
   "outputs": [],
   "source": [
    "def train_and_accuracy_gen(model, label, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    calculate_metrics(label, y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93a91ddf",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "93a91ddf"
   },
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "\n",
    "    param_grid_logistic_regression = {\n",
    "        'C': [0.01, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    param_grid_decission_tree_classifier = {\n",
    "        'max_depth': [None, 5, 20, 50],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "\n",
    "    param_grid_random_forest_classifier = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 50],\n",
    "        'bootstrap': [True, False],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    param_grid_gaussian_naive_bias = {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "    }\n",
    "\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    }\n",
    "\n",
    "    param_grid_knn = {\n",
    "        'n_neighbors': [100, 500, 700, 900, 1100, 1500],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    param_grid_ada_boost = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.5, 1.0],\n",
    "        'estimator': [\n",
    "            DecisionTreeClassifier(max_depth=1),\n",
    "            DecisionTreeClassifier(max_depth=3)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.3, 0.5],\n",
    "    }\n",
    "\n",
    "    param_grid_grad_boost = {\n",
    "      'n_estimators': [50, 100, 200],\n",
    "      'learning_rate': [0.01, 0.1, 0.2],\n",
    "      'max_depth': [3, 5, 7],\n",
    "      'random_state': [42]\n",
    "    }\n",
    "\n",
    "    param_grid_ann = {\n",
    "        'model__n_neurons': [32, 64, 128],\n",
    "        'model__activation': ['relu', 'tanh'],\n",
    "        'epochs': [10, 20],\n",
    "        'batch_size': [16, 32, 64]\n",
    "    }\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.feature_path = file_path\n",
    "        self.feature_df = self.get_feture()\n",
    "        self.X, self.y = self.split_feture_and_target()\n",
    "        self.y = self.map_zero_to_n() # mapping y zero to number of class to make it usable for some modles i.e. xgaboost\n",
    "        self.number_of_categories = self.get_number_of_categories()\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.get_scale_and_test_train_split()\n",
    "\n",
    "    # data read and processing section\n",
    "    def remove_outliear(self, feature_df):\n",
    "        iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "        outliers = iso.fit_predict(feature_df)\n",
    "        data_cleaned = feature_df[outliers == 1]\n",
    "\n",
    "        return data_cleaned\n",
    "\n",
    "    def get_feture(self):\n",
    "        feature_df = pd.read_csv(self.feature_path)\n",
    "        feature_df = feature_df.iloc[:, 1:] # remove index\n",
    "\n",
    "        return self.remove_outliear(feature_df)\n",
    "\n",
    "    def split_feture_and_target(self):\n",
    "        X = self.feature_df.iloc[:, :-1]\n",
    "        y = self.feature_df.iloc[:, -1]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_scale_and_test_train_split(self):\n",
    "        #Scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaled_fature = scaler.fit_transform(self.X)\n",
    "\n",
    "        #test train split\n",
    "        return train_test_split(scaled_fature, self.y, train_size=.20, random_state=42, stratify=self.y)\n",
    "\n",
    "    def map_zero_to_n(self):\n",
    "        unique_values = {val: idx for idx, val in enumerate(self.y.unique())}\n",
    "        y_mapped = self.y.map(unique_values)\n",
    "\n",
    "        return y_mapped\n",
    "\n",
    "    def get_number_of_categories(self):\n",
    "        return len(self.y.unique())\n",
    "\n",
    "    def onehot_encode(self):\n",
    "        self.y_train = to_categorical(self.y_train, num_classes = self.number_of_categories)\n",
    "        self.y_test = to_categorical(self.y_test, num_classes = self.number_of_categories)\n",
    "\n",
    "        print(self.y_train.shape)\n",
    "\n",
    "    # Cross validation section\n",
    "    def kfold_cross_validation(self, model, n_splits):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        kfold_score = cross_val_score(model, self.X, self.y, cv=kf)\n",
    "\n",
    "        print(\"K-fold cross validaiton scores:\", kfold_score)\n",
    "\n",
    "    def stratified_cross_validation(self, model, n_splits):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        skfold_score = cross_val_score(model, self.X, self.y, cv=skf)\n",
    "\n",
    "        print(\"Straified cross validation scores:\", skfold_score)\n",
    "\n",
    "    def cross_validation(self, model, n_splits):\n",
    "        self.kfold_cross_validation(model, n_splits)\n",
    "        self.stratified_cross_validation(model, n_splits)\n",
    "\n",
    "    # Hyper parameter tuning\n",
    "\n",
    "    def gridSerach(self, estimator, param_grid):\n",
    "        print(\"==== Grid Search: =====\")\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3, verbose=0)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", grid_search.best_params_)\n",
    "        print(\"Best score found: \", grid_search.best_score_)\n",
    "\n",
    "        return grid_search\n",
    "\n",
    "    def randomSearch(self, estimator, param_grid):\n",
    "        print(\"\\n==== Random Search: =====\")\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=500, cv=3, random_state=42)\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", random_search.best_params_)\n",
    "        print(\"Best score found: \", random_search.best_score_)\n",
    "\n",
    "        return random_search\n",
    "\n",
    "    def hyper_parameter_tuning(self, model, param_grid):\n",
    "        grid_search = self.gridSerach(model, param_grid)\n",
    "        random_search = self.randomSearch(model, param_grid)\n",
    "\n",
    "        return grid_search if grid_search.best_score_ > random_search.best_score_ else random_search\n",
    "\n",
    "    # Models section\n",
    "    def run_logistic_regression_model(self):\n",
    "        print(\"=============== 1. Logistic Regression Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(LogisticRegression(), self.param_grid_logistic_regression)\n",
    "        lrm = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(lrm, \"1. Logistic regression\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(lrm, 10)\n",
    "\n",
    "    def run_decission_tree_classifier_model(self):\n",
    "        print(\"=================2. Decission Tree Classifier Section: ================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(DecisionTreeClassifier(), self.param_grid_decission_tree_classifier)\n",
    "        dt = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(dt, \"2. Decission Tree Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(dt, 10)\n",
    "\n",
    "    def run_random_forest_classifier_model(self):\n",
    "        print(\"=================== 3. Random Forest Classifier Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(RandomForestClassifier(), self.param_grid_random_forest_classifier)\n",
    "        rfc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(rfc, \"3.  Random Forest Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(rfc, 10)\n",
    "\n",
    "    def run_gaussian_naive_bias_classifier_model(self):\n",
    "        print(\"=================== 4. Gaussian Naive Bias Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(GaussianNB(), self.param_grid_gaussian_naive_bias)\n",
    "        gnb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(gnb, \"4. Gaussian Naive Bias Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(gnb, 10)\n",
    "\n",
    "\n",
    "    def run_support_vector_classifier_model(self):\n",
    "        print(\"=================== 5. Support Vector Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(SVC(), self.param_grid_svc)\n",
    "        svc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(svc, \"5. Support Vector Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(svc, 10)\n",
    "\n",
    "\n",
    "    def run_knn_classifier_model(self):\n",
    "        print(\"=================== 6. K-Nearest Neighbors Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(KNeighborsClassifier(), self.param_grid_knn)\n",
    "        knn = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(knn, \"6. K-Nearest Neighbors\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(knn, 10)\n",
    "\n",
    "    def run_ada_boost_classifier_model(self):\n",
    "        print(\"=================== 7. Ada Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(AdaBoostClassifier(), self.param_grid_ada_boost)\n",
    "        abc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(abc, \"7. Ada Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(abc, 10)\n",
    "\n",
    "    def run_xg_boost_classifier_model(self):\n",
    "        print(\"=================== 8. XG Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(XGBClassifier(), self.param_grid_xgb)\n",
    "        xgb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(xgb, \"8. XG Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(xgb, 10)\n",
    "\n",
    "    def run_gradient_boost_classifier_model(self):\n",
    "        print(\"=================== 9. Gradient Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(GradientBoostingClassifier(), self.param_grid_grad_boost)\n",
    "        gb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(gb, \"9. Gradient Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(gb, 10)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ann(n_neurons=128, activation='relu'):\n",
    "        model = Sequential()\n",
    "        # Input layer\n",
    "        model.add(Dense(n_neurons, activation=activation, input_shape=(24,)))\n",
    "\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "\n",
    "        # Output layer (example for binary classification)\n",
    "        model.add(Dense(units=15, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def run_ann_model(self):\n",
    "        print(\"=================== 10. Artificial Neural Net Section: ===================\")\n",
    "\n",
    "        y_train_tmp = self.y_train\n",
    "        y_test_tmp = self.y_test\n",
    "\n",
    "        self.onehot_encode()\n",
    "\n",
    "        model = KerasClassifier(build_fn=self.build_ann, verbose=1, epochs = 50, batch_size = 100)\n",
    "\n",
    "        # tuned_model = self.hyper_parameter_tuning(model, self.param_grid_ann)\n",
    "        # ann = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(model, \"10. Artificial Neuralnet\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        # self.cross_validation(ann, 10)\n",
    "\n",
    "        self.y_train = y_train_tmp\n",
    "        self.y_test = y_test_tmp\n",
    "\n",
    "    def driver(self):\n",
    "        # self.run_logistic_regression_model()\n",
    "        # self.run_decission_tree_classifier_model()\n",
    "        # self.run_random_forest_classifier_model()\n",
    "        # self.run_gaussian_naive_bias_classifier_model()\n",
    "        # self.run_support_vector_classifier_model()\n",
    "        # self.run_knn_classifier_model()\n",
    "        # self.run_ada_boost_classifier_model()\n",
    "        # self.run_xg_boost_classifier_model()\n",
    "        self.run_ann_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "xkrwkNYuf8X7",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733438705562,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "xkrwkNYuf8X7"
   },
   "outputs": [],
   "source": [
    "ROOT = '/content/drive/MyDrive/MLClass/MLProject/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57df36a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1733438706203,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "57df36a5"
   },
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(ROOT + \"features/w500_o25_features.csv\")\n",
    "#model_evaluation_pipeline.run_ann_model()\n",
    "\n",
    "#model_evaluation_pipeline.run_gradient_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "uRXth4kZvGqT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22798,
     "status": "ok",
     "timestamp": 1733438729000,
     "user": {
      "displayName": "Nadim Mahmud",
      "userId": "12866761848113428285"
     },
     "user_tz": 300
    },
    "id": "uRXth4kZvGqT",
    "outputId": "478a4bbf-14b8-4cb3-d330-7e749a290eb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== 10. Artificial Neural Net Section: ===================\n",
      "(1508, 15)\n",
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.1225 - loss: 2.6548\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2348 - loss: 2.3389\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3268 - loss: 2.0555\n",
      "Epoch 4/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3822 - loss: 1.8395\n",
      "Epoch 5/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4055 - loss: 1.6918\n",
      "Epoch 6/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4494 - loss: 1.6201\n",
      "Epoch 7/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4418 - loss: 1.5632\n",
      "Epoch 8/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4450 - loss: 1.5232\n",
      "Epoch 9/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4901 - loss: 1.4844\n",
      "Epoch 10/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5140 - loss: 1.4234\n",
      "Epoch 11/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5200 - loss: 1.4116 \n",
      "Epoch 12/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5039 - loss: 1.3856\n",
      "Epoch 13/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5622 - loss: 1.2843\n",
      "Epoch 14/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5539 - loss: 1.2866\n",
      "Epoch 15/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5700 - loss: 1.2631\n",
      "Epoch 16/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5867 - loss: 1.2043\n",
      "Epoch 17/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5960 - loss: 1.2091\n",
      "Epoch 18/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5923 - loss: 1.1557\n",
      "Epoch 19/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5972 - loss: 1.1495\n",
      "Epoch 20/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6070 - loss: 1.1296\n",
      "Epoch 21/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6429 - loss: 1.0696\n",
      "Epoch 22/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6465 - loss: 1.0593\n",
      "Epoch 23/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6198 - loss: 1.0770\n",
      "Epoch 24/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6347 - loss: 1.0512\n",
      "Epoch 25/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6517 - loss: 1.0032\n",
      "Epoch 26/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6504 - loss: 0.9914\n",
      "Epoch 27/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6605 - loss: 0.9757\n",
      "Epoch 28/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6733 - loss: 0.9471\n",
      "Epoch 29/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6723 - loss: 0.9470\n",
      "Epoch 30/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6872 - loss: 0.9018\n",
      "Epoch 31/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6834 - loss: 0.9038\n",
      "Epoch 32/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6870 - loss: 0.9242\n",
      "Epoch 33/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6873 - loss: 0.8940\n",
      "Epoch 34/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7056 - loss: 0.8502\n",
      "Epoch 35/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7147 - loss: 0.8263\n",
      "Epoch 36/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7165 - loss: 0.8328\n",
      "Epoch 37/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7264 - loss: 0.7998\n",
      "Epoch 38/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7385 - loss: 0.7815\n",
      "Epoch 39/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7078 - loss: 0.8256\n",
      "Epoch 40/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7408 - loss: 0.7415\n",
      "Epoch 41/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7388 - loss: 0.7688\n",
      "Epoch 42/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7101 - loss: 0.7984\n",
      "Epoch 43/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7436 - loss: 0.7468\n",
      "Epoch 44/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7588 - loss: 0.7139\n",
      "Epoch 45/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7314 - loss: 0.7125\n",
      "Epoch 46/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7461 - loss: 0.7670\n",
      "Epoch 47/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7557 - loss: 0.7156\n",
      "Epoch 48/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7785 - loss: 0.6819\n",
      "Epoch 49/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7672 - loss: 0.6403\n",
      "Epoch 50/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7833 - loss: 0.6530\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "10. Artificial Neuralnet metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.49      0.51       416\n",
      "           1       0.55      0.65      0.60       448\n",
      "           2       0.42      0.34      0.38       418\n",
      "           3       0.42      0.61      0.50       437\n",
      "           4       0.82      0.81      0.82       348\n",
      "           5       0.39      0.43      0.41       438\n",
      "           6       0.30      0.26      0.28       408\n",
      "           7       0.58      0.17      0.27       325\n",
      "           8       0.80      0.69      0.74       394\n",
      "           9       0.64      0.86      0.73       414\n",
      "          10       0.57      0.46      0.51       351\n",
      "          11       0.53      0.43      0.47       410\n",
      "          12       0.50      0.64      0.56       447\n",
      "          13       0.72      0.54      0.62       424\n",
      "          14       0.61      0.81      0.69       354\n",
      "\n",
      "   micro avg       0.55      0.55      0.55      6032\n",
      "   macro avg       0.56      0.55      0.54      6032\n",
      "weighted avg       0.55      0.55      0.54      6032\n",
      " samples avg       0.55      0.55      0.55      6032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_pipeline.run_ann_model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
